{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T10:34:26.554515Z",
     "start_time": "2020-06-08T10:34:26.370Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate <- function(TrueLabelsPath, PredLabelsPath, Indices = NULL){\n",
    "  \"\n",
    "  Script to evaluate the performance of the classifier.\n",
    "  It returns multiple evaluation measures: the confusion matrix, median F1-score, F1-score for each class, accuracy, percentage of unlabeled, population size. \n",
    "  \n",
    "  The percentage of unlabeled cells is find by checking for cells that are labeled 'Unassigned', 'unassigned', 'Unknown', 'unknown', 'Nodexx', 'rand', or 'ambiguous'.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  TrueLabelsPath: csv file with the true labels (format: one column, no index)\n",
    "  PredLabelsPath: csv file with the predicted labels (format: one column, no index)\n",
    "  Indices: which part of the csv file should be read (e.g. if more datasets are tested at the same time) (format: c(begin, end))\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  Conf: confusion matrix\n",
    "  MedF1 : median F1-score\n",
    "  F1 : F1-score per class\n",
    "  Acc : accuracy\n",
    "  PercUnl : percentage of unlabeled cells\n",
    "  PopSize : number of cells per cell type\n",
    "  \"\n",
    "  \n",
    "  true_lab <- unlist(read.csv(TrueLabelsPath))\n",
    "  pred_lab <- unlist(read.csv(PredLabelsPath))\n",
    "  \n",
    "  if (! is.null(Indices)){\n",
    "    true_lab <- true_lab[Indices]\n",
    "    pred_lab <- pred_lab[Indices]\n",
    "  }\n",
    "  \n",
    "  unique_true <- unlist(unique(true_lab))\n",
    "  unique_pred <- unlist(unique(pred_lab))\n",
    "  \n",
    "  unique_all <- unique(c(unique_true,unique_pred))\n",
    "  conf <- table(true_lab,pred_lab)\n",
    "  pop_size <- rowSums(conf)\n",
    "  \n",
    "  pred_lab = gsub('Node..','Node',pred_lab)\n",
    "  \n",
    "  conf_F1 <- table(true_lab,pred_lab,exclude = c('unassigned','Unassigned','Unknown','rand','Node','ambiguous','unknown'))\n",
    "\n",
    "  F1 <- vector()\n",
    "  sum_acc <- 0\n",
    "  \n",
    "  for (i in c(1:length(unique_true))){\n",
    "    findLabel = colnames(conf_F1) == row.names(conf_F1)[i]\n",
    "    if(sum(findLabel)){\n",
    "      prec <- conf_F1[i,findLabel] / colSums(conf_F1)[findLabel]\n",
    "      rec <- conf_F1[i,findLabel] / rowSums(conf_F1)[i]\n",
    "      if (prec == 0 || rec == 0){\n",
    "        F1[i] = 0\n",
    "      } else{\n",
    "        F1[i] <- (2*prec*rec) / (prec + rec)\n",
    "      }\n",
    "      sum_acc <- sum_acc + conf_F1[i,findLabel]\n",
    "    } else {\n",
    "      F1[i] = 0\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  pop_size <- pop_size[pop_size > 0]\n",
    "  \n",
    "  names(F1) <- names(pop_size)\n",
    "  \n",
    "  med_F1 <- median(F1)\n",
    "  \n",
    "  total <- length(pred_lab)\n",
    "  num_unlab <- sum(pred_lab == 'unassigned') + sum(pred_lab == 'Unassigned') + sum(pred_lab == 'rand') + sum(pred_lab == 'Unknown') + sum(pred_lab == 'unknown') + sum(pred_lab == 'Node') + sum(pred_lab == 'ambiguous')\n",
    "  per_unlab <- num_unlab / total\n",
    "  \n",
    "  acc <- sum_acc/sum(conf_F1)\n",
    "  \n",
    "  result <- list(Conf = conf, MedF1 = med_F1, F1 = F1, Acc = acc, PercUnl = per_unlab, PopSize = pop_size)\n",
    "  \n",
    "  return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T10:37:35.503487Z",
     "start_time": "2020-06-08T10:37:35.483Z"
    }
   },
   "outputs": [],
   "source": [
    "setwd(\"/Volumes/LACIE_SHARE/project/2020-06-08-svmPipeline_corces2016train_buenrostro2018test/output/enh0.3_intNo\")\n",
    "TrueLabelsPath <- \"./SVM_true.csv\"\n",
    "PredLabelsPath <- \"./SVM_pred.csv\"\n",
    "OutputDir <- \"./\"\n",
    "ToolName <- \"SVM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T10:37:36.210350Z",
     "start_time": "2020-06-08T10:37:35.836Z"
    }
   },
   "outputs": [],
   "source": [
    "results <- evaluate(TrueLabelsPath, PredLabelsPath)\n",
    "dir.create(file.path(OutputDir, \"Confusion\"))\n",
    "dir.create(file.path(OutputDir, \"F1\"))\n",
    "dir.create(file.path(OutputDir, \"PopSize\"))\n",
    "dir.create(file.path(OutputDir, \"Summary\"))\n",
    "write.csv(results$Conf, file.path(OutputDir, \"Confusion\", paste0(ToolName, \".csv\")))\n",
    "write.csv(results$F1, file.path(OutputDir, \"F1\", paste0(ToolName, \".csv\")))\n",
    "write.csv(results$PopSize, file.path(OutputDir, \"PopSize\", paste0(ToolName, \".csv\")))\n",
    "df <- data.frame(results[c(\"MedF1\", \"Acc\", \"PercUnl\")])\n",
    "write.csv(df, file.path(OutputDir, \"Summary\", paste0(ToolName, \".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:ATACseq_SnapATAC]",
   "language": "R",
   "name": "conda-env-ATACseq_SnapATAC-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
